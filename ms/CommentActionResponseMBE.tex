\documentclass{article}
\usepackage[margin=1.25in,top=1in,bottom=1in]{geometry}
\usepackage{float}
\usepackage{setspace}
%\usepackage{lineno}
%\linenumbers
\linespread{2} 
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\newcommand{\ignore}[1]{}
%---------------------------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------------------------
\begin{document}

\section*{Associate Editor}
Editorsâ€™ comments to the author:
The comments and recommendations from 2 expert reviewers are now available for your manuscript. These reviewers judged the reported methods to be of medium significance, and the potential scientific impact of your work to be medium. They found that the manuscript needs substantial improvement in text and additional data analysis. In particular, the small number of real applications, the lack of what information about them and what they are supposed to highlight, and the question of how to apply the new metrics to improve epidemiological inference were found problematic. Editors generally agree with their concerns and recommendations and recommend rejection of this manuscript.

\subsection*{Action}
\textcolor{blue}{
The feedback broadly indicates to me that we need to either strengthen the relationship to epi here, or cut that back and strengthen the theoretical side. The method is prospective, so I am struggling to propose further epi applications at the moment. I do have some ideas for the theoretical side though:
\begin{itemize}
    \item Can estimate all dates under the coalescent like we did for the birth-death?
    \item I will include an explanatory figure for how I propose that dates influence inference more definitively than sequence.
    \item Maybe we can do some kind of application where the track $W_D$ and $W_S$ as we increase sampling proportion?
    \item Maybe we can talk about $W_S$ as giving us the strength of topological/genomic signal relative to epi in the analyses?
\end{itemize}}

\section*{Reviewer: 1}

This manuscript presents an interesting method for quantifying the relative contributions of genetic sequences and sampling dates on the estimation of birth-death model parameters, specifically the basic reproduction number.  This issue has received increasing attention with the recent uptake of phylodynamic methods for monitoring the local transmission dynamics of SARS-CoV-2.  

The underlying premise is fairly straight-forward: either or both types of data are censored and we compare the resulting posterior sample of the parameter of interest (i.e., $R_{0}$) to the results from the complete data.  The manuscript would benefit from some more clarification and methdological details on how, for example, the inverse empirical distributions are derived from MCMC results.  (See below for other specific examples.)  


The authors present results on classification, but it is very unclear what is being classified and how.  For example, what are the labels being predicted?  What are the inputs, and what sort of classifier are they using?  How is that classifier being trained?

\subsection*{Action}
\textcolor{blue}{I think using the word classifier is an issue here, but at the same time I feel we made it pretty clear in the manuscript. In any event, I'm not so sure the classification component is as useful as quantifying signal, so I could leave this out. I got a comment at MCEB about signing $W_{\cdot}$ to reflect the difference in posterior means like we originally had. Maybe we can resume this to add detail in lieu of a 'classifier'?}

In addition, the manuscript provides results on actual SARS-CoV-2 data, but this section ("Empirical Results") is not well developed.  Please provide more context on the SARS-CoV-2 transmission clusters being analysed.  The original study should be cited here! How were these clusters extracted from the data?  What are the characteristics of these clusters?  What populations, regions were they sampled in?  What sampling dates do they span?  What lineages do they correspond to?  Can these clusters be displayed in the context of a phylogeny?  What estimates of $R_{0}$ did the original study derive from these clusters?  How might the conclusions of that study be affected by the difference in sensitivity to sequence data?  Why are only two clusters being described here?  Lane et al. identified 595 clusters.  Why these two clusters in particular?  How were they selected?

\subsection*{Action}
\textcolor{blue}{Pretty annoying because the are just for illustration and we did cite the original study! Still, I agree we could do more with the application section if we can find a way to show how wasserstein is useful.}

Finally, this study would be far more impactful if it can provide guidelines for when public health agencies should proceed with fast-tracked sequencing of samples for phylodynamic analysis.  A more extensive comparative study of SARS-CoV-2 transmission clusters (i.e., more than two) may help in this regard.  In what cases can we anticipate that sequence data will provide substantially more information than case data (sampling dates) alone?  Is it possible to predict this before collecting the sequence data, or is it solely contingent on the amount of genetic variation (divergence among sequences)?  If it is not possible to determine the value of sequence data *a priori*, then the manuscript should at least make this limitation clear.

\subsection*{Action}
\textcolor{blue}{
Again, I thought we did make it clear that we cannot predict 'classification' a-priori and having heaps of site patterns does not guarantee $W_{S} < W_{D}$.}

- Art Poon

\subsection*{Reviewer 1 Specific comments (can address for resub.)}
\begin{footnotesize}

* line 24: "the average number of secondary infections [from the index case] in a[n] [otherwise] fully susceptible population)" - I suggest these edits for clarification

* line 25: "time-stamped genome sequences" - this phrase may be unfamiliar to non-specialists.  I suggest leading with something like "sequences annotated with sample collection dates"

* lines 50-51: "The coalescent also assumes a low sampling proportion relative to population size [...]"  To be fair, this limitation is specifically the case for Kingman's coalescent.  Subsequent extensions of the coalescent are designed to relax this assumption, such as the `$\Lambda$` coalescent.

* Table 1 seems unnecessary - the same information can be conveyed in a couple of sentences in text.

* Figure 1A, for these boxes please display only two sequences, increasing the text size, and use a fixed-width font

* Figure 1B, none of the text is legible, especially the formula for $W_{x}$ and the tick labels.  Moreover, displaying the formula here seems unnecessary to me, since the metric is being described in the text.

* Figure 1C, is it really necessary for $W_{S}$ and $W_{D}$ axes to go all the way to 1.25?  There is an awful lot of unused space in this plot.  All text should be rendered with larger fonts.

* line 75, please provide some description of the new MCMC operator and its relevance to this particular application.

* line 82, explain how the inverse empirical distribution functions are being derived from posterior samples of $R_{0}$.  

* line 116, "under the birth-death Volz and Frost (2014)."  It looks like the authors used `\citet{}` instead of `\citep{}` here.  This should probably be written as "under the birth-death model (Volz and Frost, 2014)."

* Figure 2, again the text is unreadable.  The default settings for ggplot2 (assuming this is the framework being used) are really awful for this reason.

* lines 123-124:  "We also tested the accuracy of classification using the Wasserstein metric by subsampling and reclassifying each posterior $R_0$ distribution 100 times."  This is unclear.  What are the labels?  What classifier are the authors using?  What is the training, testing and validation procedure?  Moreover, the section on "Validating the Wasserstein Metric" (lines 201-211) is very unclear.

* Figure 3 is so much better than the previous figures.  My only suggestion would be to use discrete-valued legends for B and C, rather than a continuous scale (i.e., "Site Patterns" and "Relative Date Span") that is misleading.

* lines 134-135: "On the other hand, the date span does not follow [an] equivalent trend"

* line 177, why is this section titled "Supplementary Methods"?  Doesn't MBE have a standard Methods section?

* line 186, there is a LaTeX error in rendering "subs/site/time" - it looks like the authors forgot a closing curly brace in association with the subscript.  This should be entered as `$10^{-5}$ subs/site/time` so that the units are rendered in plain text.

* lines 186-187, Seq-Gen should be referenced in association with the previous statement: "For each outbreak, we simulated [...]"

* line 190, "We analyzed each under a birth-death model [...]"  Using BEAST?  Also, "Uniform" should be rendered in text mode, not math mode, i.e., `$\textrm{Uniform}[0,5]$`.

* line 194, "We analysed two similar SARS-CoV-2 datasets taken from (Lane et al., 2021)."  Should use `\citet` here instead of `\citep`.

* line 196, "Inv - Gamma", is this meant to be a hyphenation of inverse gamma?  Rendered the way it is, it looks like you are subtracting Gamma from some value Inv.

* lines 199-200:  "between the index case [to] the first putative transmission event"

* line 202, provide a reference or URL for the R package *transport* (it would also help to render the package name in italics)

* line 208, extra right parentheses in association with `$d_{SD}$`

* line 211, "Classification is wholly reliabl[e] above this threshold"

* References should omit URL fields.  Some capitalization needs to be fixed, e.g., Lane et al. "covid-19" should be "COVID-19", "victoria" should be "Victoria".  Easiest remedy is to protect the title fields in the .bib file with curly braces.

* the GitHub repository is fairly well documented and the R scripts are commented, although they do not adhere to a consistent code style (e.g., number of spaces used to indent lines, spaces between arguments) and employ several hard-coded file paths.  I am surprised that simulating the sequence alignments consumes so much RAM (about 50GB, as noted in the README file).  Is this a problem specific to Seq-Gen?
\end{footnotesize}

\section*{Reviewer: 2}

Comments to the Author
In a phylodynamic analysis, it may not be clear how much of the "signal" is coming from different aspects of the data.  This manuscript presents a straightforward framework that quantitatively compares how much signal comes from the sequence data itself versus simply the dates of the samples.  There is a lot to like: the central issue makes sense, the approach is intuitive, the presentation is concise, and the text and figures explain things clearly.  Despite these many positives, though, I am left wondering what the utility of the work actually is.

The main message seems to be, sometimes sequences contribute more and sometimes dates contribute more.  But so what?  If I have data for a cluster of interest and compute the Wasserstein metrics, how can I use them to improve my analysis, or what epidemiological insight do I gain?  Alternatively, can your approach provide more general conclusions about when different data or methods are expected to be more informative?  Such as, do sequence data contribute more for some models than others, e.g. for models with different R0 for different compartments/states?  Or optimal sampling design (from lines 35-37 and 169): e.g. Are the sequence data more informative if 10\% of cases are sampled via random surveillance versus contact tracing?  How does one strategically subsample dense sequence data in order to get equally good results more efficiently?  Or something empirical about SARS-CoV-2, e.g. when is it worth bothering with phylodynamics versus just analyzing case counts (i.e. dates for all cases, not just those samples with sequences)?  These are just examples, and I don't mean that the authors need to tackle all or any of these specific topics.  But I do think that the significance of the work would be greatly increased if this manuscript not only introduced a tool, but also applied it to some specific epidemiological question, whether theoretical or empirical.

\subsection*{Action}
\textcolor{blue}{Rough but I get it, we need to pitch an application here. I want to suggest doing simulations with am additional much lower sampling proportion and see how $W_{\bullet}$ changes over $p$. Does error absolute error vary with $p$ too?.}
\begin{footnotesize}
That was my main criticism.  Below are just some smaller possible improvements.

I found Fig 1 quite helpful, and I referred to it many times while reading the rest of the text.  But for panel B:  I would generally expect that the D and S distributions would be broader than F (though not as broad as N).  Also, I would not generally expect that D would be in such disagreement with F.  And, the bars indicating $W_{D}$ etc at first made me think that only the means of the distributions are used.  Is there a way to indicate better that the whole distribution contributes, to make clear the contribution of not only the offset but also the change in uncertainty?
\end{footnotesize}
Including sampling proportion in the simulation study is highly relevant, but two improvements seem worthwhile.  First, the simulation results show a small effect of sampling proportion (e.g. Fig 3D) for p = 0.5, but many real datasets will have much lower values.  For example, overall sars-cov-2 proportions (total gisaid seqs / total confirmed cases) are only 4\% for the US and 12\% for the UK.  So it would be helpful to see how information is lost with much lower p.  And second, p = 0.5 could mean two things: dropping half the data (as done here) or that the same number of samples covers only half the cases.  The latter seems equally relevant.

\subsection*{Action}
\textcolor{blue}{No idea what Rev2. is on about with the half data vs half cases point. But this does support the idea of looking into lower sampling more to look more into sampling.}

\begin{footnotesize}
It is certainly good to include empirical applications, but these are rather thin.  They do show how different datasets can show different metric values.  But there is no real context provided---please refer to my main comment above about rethinking what is actually being learned from the study.
\end{footnotesize}

I appreciated the intuitive interpretation of when the different data aspects contribute (lines 159-163), but it was the opposite of my own intuition!  I would think that sequence data alone would provide much information about tip dates (at least, along with some clock model assumptions), compensating for date data being omitted.  And it's not obvious how date data alone constrains tree topology, particularly when there are unsampled infections.  So I think this is a valuable section that could be better-developed.

\subsection*{Action}
\textcolor{blue}{I need to include that figure I've been thinking of.}

\begin{footnotesize}
Table 1 is not referred to anywhere, and its terminology isn't even consistent with language used elsewhere.

The Fig 2 facet labels are too cryptic.
\end{footnotesize}

\end{document}